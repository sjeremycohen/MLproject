---
title: "HW2.Rmd"
output: html_document
date: "2022-12-08"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Loading the Data Again
As a reminder, during EDA we compressed a field, LAW_CAT_CD, which had 3 classes of crime, FELONY, MISDEMEANOR, VIOLATION into a binary categorization called FELONY, 1 if a felony, 0 if not.This is our class field.

```{r Load Data}
cdata <- read.csv("cdata.csv")
cdata <- subset(cdata, select = -c(X))
length(cdata)
```
We'll use the same Error Metrics core function as we did in HW1. It will serve us just as well returning info about our ensemble methods as it did our plain classifiers.
```{r Error Metrics}
err_metric=function(CM)
{
  TN =CM[1,1]
  TP =CM[2,2]
  FP =CM[1,2]
  FN =CM[2,1]
  precision =(TP)/(TP+FP)
  specificity_score = (TN) / (TN + FP)
  recall_score =(FP)/(FP+TN)
  f1_score=2*((precision*recall_score)/(precision+recall_score))
  accuracy_model  =(TP+TN)/(TP+TN+FP+FN)
  False_positive_rate =(FP)/(FP+TN)
  False_negative_rate =(FN)/(FN+TP)
  print(paste("Accuracy of the model: ",round(accuracy_model,2)))
  print(paste("Specificity value of the model: ",round(specificity_score,2)))
  print(paste("Precision value of the model: ",round(precision,2)))
  print(paste("Recall/Sensitivity value of the model: ",round(recall_score,2)))
}
```

# Random Forest


```{r RF TT-Split}
set.seed(777)
train_pct = .05
split <- sample(c(TRUE, FALSE), nrow(cdata), replace=TRUE, prob=c(train_pct,(1-train_pct)))

cdTrain <- cdata[split,]
cdTest <- cdata[!split,]
nrow(cdTrain)
nrow(cdTest)
```
```{r RF}
library(randomForest)

# Run RandomForest on the CDTrain dataset
rf_m <- suppressWarnings(randomForest(FELONY ~ ., data = cdTrain, ntree = 5))
rf_p <- predict(rf_m, newdata = cdTest)
rf_var <- paste("Variance: ", var(rf_p))
rf_p <- ifelse(rf_p > 0.5,1,0) # Probability check
CM = table(cdTest[,124] , rf_p)
print(CM)
err_metric(CM)
print(rf_var)
```
```{r RF ROCAUC}
library(pROC)
rf_roc_score=roc(cdTest$FELONY, rf_p) #AUC score
plot(rf_roc_score ,main ="ROC curve -- LOOCV ")
print(rf_roc_score$auc)
```

# Cross Validation

```{r CV TT-Split}
set.seed(777)
train_pct = .3
split <- sample(c(TRUE, FALSE), nrow(cdata), replace=TRUE, prob=c(train_pct,(1-train_pct)))

cdTrain <- cdata[split,]
cdTest <- cdata[!split,]
nrow(cdTrain)
nrow(cdTest)
```

```{r CV}
# Perform cross-validation using the caret package
library(caret)
fitControl <- trainControl(method = "cv", number = 10) # 10-fold cross-validation
cv_m <- suppressWarnings(train(FELONY ~ ., data = cdTrain, method = "glm", trControl = fitControl))

# Evaluate the model on the test set
cv_p <- predict(cv_m, cdTest)
cv_var <- paste("Variance: ", var(cv_p))
cv_p <- ifelse(cv_p > 0.5,1,0) # Probability check
CM = table(cdTest$FELONY , cv_p)
print(CM)
err_metric(CM)
print(cv_var)
```

```{r CV ROCAUC}
library(pROC)
cv_roc_score=roc(cdTest$FELONY, cv_p) #AUC score
plot(cv_roc_score ,main ="ROC curve -- Cross Validation ")
print(cv_roc_score$auc)
```
# Leave-One-Out Cross Validation (LOOCV)
Next we'll run Leave One Out Cross-Validation (LOOCV) using GLM as our base model.
Because LOOCV has to run n times, where n is the length of the dataset, we'll be running this on an extremely small sample of our data, only 1%. This will still take some time to run.

```{r LOOCV TT-Split}
set.seed(777)
train_pct = .01
split <- sample(c(TRUE, FALSE), nrow(cdata), replace=TRUE, prob=c(train_pct,(1-train_pct)))

cdTrain <- cdata[split,]
cdTest <- cdata[!split,]
nrow(cdTrain)
nrow(cdTest)
```

```{r LOOCV}
# Perform cross-validation using the caret package
library(caret)
fitControl <- trainControl(method = "LOOCV")
loocv_m <- suppressWarnings(train(FELONY ~ ., data = cdTrain, method = "glm", trControl = fitControl))

# Evaluate the model on the test set
loocv_p <- predict(loocv_m, cdTest)
loocv_var <- paste("Variance: ", var(loocv_p))
loocv_p <- ifelse(loocv_p > 0.5,1,0) # Probability check
loocv_CM = table(cdTest$FELONY , loocv_p)
print(loocv_CM)
err_metric(loocv_CM)
print(loocv_var)
```
```{r LOOCV ROCAUC}
library(pROC)
loocv_roc_score=roc(cdTest$FELONY, loocv_p) #AUC score
plot(loocv_roc_score ,main ="ROC curve -- LOOCV ")
print(loocv_roc_score$auc)
```
